{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Добавляем индексы к исходным данным\n",
    "    indices = np.arange(data.shape[0]).reshape(-1, 1)\n",
    "    indices = np.repeat(indices, data.shape[1], axis=1)  # Повторяем индексы для всех временных шагов\n",
    "    indices = indices[:, :, np.newaxis]\n",
    "    data_with_indices = np.concatenate([data, indices], axis=2)\n",
    " \n",
    "    # Cтроки с пропусками \n",
    "    schaefer_mask = np.isnan(data_with_indices[:, :, :-1]).any(axis=2)\n",
    "    \n",
    "    # Cтроки без пропусков (Brainnetome)\n",
    "    brainnetome_data = data_with_indices[~schaefer_mask.any(axis=1)]\n",
    "    \n",
    "    # Schaefer200\n",
    "    schaefer_data = data_with_indices[schaefer_mask.any(axis=1)]\n",
    "    \n",
    "    # Сохраняем индексы для восстановления\n",
    "    brainnetome_indices = brainnetome_data[:, 0, -1]  \n",
    "    schaefer_indices = schaefer_data[:, 0, -1]\n",
    "    \n",
    "    # Убираем столбец с индексами\n",
    "    brainnetome_data = brainnetome_data[:, :, :-1]\n",
    "    schaefer_data = schaefer_data[:, :, :-1]\n",
    "    \n",
    "    # Из Schaefer200 нужны только первые 200 признаков\n",
    "    schaefer_data = schaefer_data[:, :, :200]\n",
    "\n",
    "    # Нормализация данных\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Применяем нормализацию к каждому временному шагу\n",
    "    brainnetome_data = scaler.fit_transform(brainnetome_data.reshape(-1, brainnetome_data.shape[2])).reshape(brainnetome_data.shape)\n",
    "    schaefer_data = scaler.fit_transform(schaefer_data.reshape(-1, schaefer_data.shape[2])).reshape(schaefer_data.shape)\n",
    "\n",
    "    # PCA для приведения к одинаковому размеру\n",
    "    pca = PCA(n_components=120)\n",
    "    \n",
    "    # Применяем PCA отдельно для каждого временного шага (поэтому нужно развернуть массив)\n",
    "    brainnetome_data = pca.fit_transform(brainnetome_data.reshape(-1, brainnetome_data.shape[2])).reshape(brainnetome_data.shape[0], brainnetome_data.shape[1], 120)\n",
    "    schaefer_data = pca.fit_transform(schaefer_data.reshape(-1, schaefer_data.shape[2])).reshape(schaefer_data.shape[0], schaefer_data.shape[1], 120)\n",
    "    \n",
    "    # Объединение данных по исходным индексам\n",
    "    combined_data = np.zeros((data.shape[0], data.shape[1], 120))  \n",
    "    combined_data[brainnetome_indices.astype(int)] = brainnetome_data\n",
    "    combined_data[schaefer_indices.astype(int)] = schaefer_data\n",
    "    \n",
    "    return combined_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_label(data, scaler=None, n=None, init=None, algorithm=None, n_clusters=20):\n",
    "    common_brain_region_data = data[:, :, 0] \n",
    "    model = make_pipeline(\n",
    "        scaler,\n",
    "        MiniBatchKMeans(n_clusters=21, random_state=42, init='random')\n",
    "    )\n",
    "\n",
    "    model.fit(common_brain_region_data)\n",
    "    cluster_distances = model.transform(common_brain_region_data)\n",
    "    labeling = np.zeros(len(data), dtype=int)\n",
    "    leftover_indexes = np.arange(len(data))\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        distances_from_current_cluster_center = cluster_distances[:, i]\n",
    "        if len(distances_from_current_cluster_center) > 16:\n",
    "            top16 = np.argpartition(distances_from_current_cluster_center, 16)[:16]\n",
    "            labeling[leftover_indexes[top16]] = i\n",
    "            cluster_distances = np.delete(cluster_distances, top16, axis=0)\n",
    "            leftover_indexes = np.delete(leftover_indexes, top16)\n",
    "        else:\n",
    "            labeling[leftover_indexes] = i\n",
    "\n",
    "    # Оценка (коэффициент силуэта)\n",
    "    silhouette_avg = silhouette_score(common_brain_region_data, labeling)\n",
    "    results['scaler'].append(scaler)\n",
    "    results['n_clusters'].append(n)\n",
    "    results['metric'].append(silhouette_avg)\n",
    "    results['inits'].append(init)\n",
    "    results['algorithms'].append(algorithm)\n",
    "    return labeling, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = [StandardScaler(), MinMaxScaler()]\n",
    "param_model = {\n",
    "    'n_clusters': [i for i in range(3, 100, 2)],\n",
    "    'inits': ['k-means++', 'random'],\n",
    "    'algorithms': ['lloyd', 'elkan']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'scaler': [],\n",
    "    'n_clusters': [],\n",
    "    'metric': [],\n",
    "    'inits': [],\n",
    "    'algorithms': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ndarray\n",
      "shape:  (320, 10, 120)\n",
      "strides:  (9600, 960, 8)\n",
      "itemsize:  8\n",
      "aligned:  True\n",
      "contiguous:  True\n",
      "fortran:  False\n",
      "data pointer: 0x2382c4e4040\n",
      "byteorder:  little\n",
      "byteswap:  False\n",
      "type: float64\n"
     ]
    }
   ],
   "source": [
    "data = np.load('../data/ts_cut/ihb.npy')\n",
    "combined_data = preprocess_data(data)\n",
    "np.info(combined_data)\n",
    "'''\n",
    "for scaler, n, init in itertools.product(scalers, param_model['n_clusters'], param_model['inits']):\n",
    "    labeling, results = cluster_and_label(combined_data, scaler, n, init, n_clusters=n)\n",
    "index = np.argmax(results['metric']) \n",
    "print(f\"scaler = {results['scaler'][index]},\\nn_clusters = {results['n_clusters'][index]},\\nmetric = {results['metric'][index]},\\ninit = {results['inits'][index]}\\n\\n\\n\")\n",
    "'''\n",
    "labeling, _ = cluster_and_label(combined_data)\n",
    "pd.DataFrame({'prediction': labeling}).to_csv('../submission_batchkmeans.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
